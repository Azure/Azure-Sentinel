{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Title: Sample Investigation - Linux, Windows, Network, Office\n",
    "### Demo version\n",
    "**Notebook Version:** 1.0<br>\n",
    "**Python Version:** Python 3.6 (including Python 3.6 - AzureML)<br>\n",
    "**Required Packages**: kqlmagic, msticpy, pandas, numpy, matplotlib, networkx, ipywidgets, ipython, scikit_learn, dnspython, ipwhois, folium, maxminddb_geolite2<br>\n",
    "**Platforms Supported**:\n",
    "- Azure Notebooks Free Compute\n",
    "- Azure Notebooks DSVM\n",
    "- OS Independent\n",
    "\n",
    "**Data Sources Required**:\n",
    "- Log Analytics - SecurityAlert, SecurityEvent (EventIDs 4688 and 4624/25), AuditLog_CL (Linux Auditd), OfficeActivity, AzureNetworkAnalytics_CL, Heartbeat\n",
    "- (Optional) - VirusTotal (with API key)\n",
    "\n",
    "## Description:\n",
    "This is an example notebook demonstrating techniques to trace the path of an attacker in an organization. Most of the steps use relatively simple _Log Analytics_ queries but it also includes a few advanced procedures such as:\n",
    "- Unpacking and decoding Linux Audit logs\n",
    "- Clustering\n",
    "\n",
    "From an initial alert (or suspect IP address) examine activity on a Linux host, a Windows and Office subscription.\n",
    "Discover malicious activity related to the ip address in each of these. \n",
    "\n",
    "The notebook is intended to illustrate the kinds of steps and data query and analysis that you might do in a real hunt or investigation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "## Table of Contents\n",
    "- [Setup and Authenticate](#setup)\n",
    "\n",
    "- [Get Alerts List](#getalertslist)\n",
    "- [Choose an Alert to investigate](#enteralertid)\n",
    "  - [Extract Properties and entities from alert](#extractalertproperties)\n",
    "  - [Basic IP Checks](#basic_ip_checks)\n",
    "    - [Check the IP Address for known C2 addresses](#check_ip_ti)\n",
    "- [See What's going on on the Affected Host - Linux](#alerthost)\n",
    "  - [Event Types collected](#linux_event_types)\n",
    "  - [Failure Events](#linux_failure_events)\n",
    "  - [Extract IPs from all Events](#linux_extract_ips)\n",
    "  - [Get Logins with IP Address Recorded](#linux_login_ips)\n",
    "  - [What's happening in these sessions?](#linux_login_sessions)\n",
    "  - [Find Distinctive Process Patterns - Clustering](#linux_proc_cluster)\n",
    "- [Alert Host Network Data](#alert_host_net)\n",
    "  - [Check Communications with Other Hosts](#comms_to_other_hosts)\n",
    "  - [GeoLocation Mapping](#geomap_lx_ips)\n",
    "  - [Have any other hosts been communicating with this address(es)?](#other_hosts_to_ips)\n",
    "- [Other Hosts Communicating with IP](#other_host_investigate)\n",
    "  - [Check Host Logons](#host_logons)\n",
    "  - [Examine a Logon Session](#examine_win_logon_sess)\n",
    "  - [Unusual Processes on Host - Clustering](#process_clustering)\n",
    "  - [Processes for Selected LogonId](#process_session)\n",
    "  - [Other Events on the Host](#other_win_events)\n",
    "- [Office 365 Activity](#o365)\n",
    "- [Summary](#summary)\n",
    "- [Appendices](#appendices)\n",
    "  - [Saving data to Excel](#appendices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>[Contents](#toc)\n",
    "# Setup\n",
    "\n",
    "Make sure that you have installed packages specified in the setup (uncomment the lines to execute)\n",
    "\n",
    "## Install Packages\n",
    "The first time this cell runs for a new Azure Notebooks project or local Python environment it will take several minutes to download and install the packages. In subsequent runs it should run quickly and confirm that package dependencies are already installed. Unless you want to upgrade the packages you can feel free to skip execution of the next cell.\n",
    "\n",
    "If you see any import failures (```ImportError```) in the notebook, please re-run this cell and answer 'y', then re-run the cell where the failure occurred.\n",
    "\n",
    "Note you may see some warnings about package incompatibility with certain packages. This does not affect the functionality of this notebook but you may need to upgrade the packages producing the warnings to a more recent version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "MIN_REQ_PYTHON = (3,6)\n",
    "if sys.version_info < MIN_REQ_PYTHON:\n",
    "    print('Check the Kernel->Change Kernel menu and ensure that Python 3.6')\n",
    "    print('or later is selected as the active kernel.')\n",
    "    sys.exit(\"Python %s.%s or later is required.\\n\" % MIN_REQ_PYTHON)\n",
    "    \n",
    "# Package Installs - try to avoid if they are already installed\n",
    "try:\n",
    "    import msticpy.sectools as sectools\n",
    "    import Kqlmagic\n",
    "    from dns import reversename, resolver\n",
    "    from ipwhois import IPWhois\n",
    "    import folium\n",
    "    import seaborn as sns\n",
    "    \n",
    "    print('If you answer \"n\" this cell will exit with an error in order to avoid the pip install calls,')\n",
    "    print('This error can safely be ignored.')\n",
    "    resp = input('msticpy and Kqlmagic packages are already loaded. Do you want to re-install? (y/n)')\n",
    "    if resp.strip().lower() != 'y':\n",
    "        sys.exit('pip install aborted - you may skip this error and continue.')\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "print('\\nPlease wait. Installing required packages. This may take a few minutes...')\n",
    "!pip install git+https://github.com/microsoft/msticpy --upgrade --user\n",
    "!pip install Kqlmagic --no-cache-dir --upgrade --user\n",
    "\n",
    "# Additional packages used in this notebook.\n",
    "!pip install dnspython --upgrade --user\n",
    "!pip install ipwhois --upgrade --user\n",
    "!pip install folium --upgrade --user\n",
    "!pip install seaborn --upgrade --user\n",
    "\n",
    "# Uncomment to refresh the maxminddb database\n",
    "# !pip install maxminddb-geolite2 --upgrade \n",
    "print('To ensure that the latest versions of the installed libraries '\n",
    "      'are used, please restart the current kernel and run '\n",
    "      'the notebook again skipping this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "MIN_REQ_PYTHON = (3,6)\n",
    "if sys.version_info < MIN_REQ_PYTHON:\n",
    "    print('Check the Kernel->Change Kernel menu and ensure that Python 3.6')\n",
    "    print('or later is selected as the active kernel.')\n",
    "    sys.exit(\"Python %s.%s or later is required.\\n\" % MIN_REQ_PYTHON)\n",
    "\n",
    "import numpy as np\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import networkx as nx\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "import msticpy.sectools as sectools\n",
    "import msticpy.nbtools as mas\n",
    "import msticpy.nbtools.kql as qry\n",
    "import msticpy.nbtools.nbdisplay as nbdisp\n",
    "from msticpy.nbtools.utility import toggle_code\n",
    "from msticpy.sectools.geoip import GeoLiteLookup\n",
    "\n",
    "import platform\n",
    "from datetime import datetime, timedelta\n",
    "from dns import reversename, resolver\n",
    "from ipwhois import IPWhois\n",
    "\n",
    "# Some of our dependencies (networkx) still use deprecated Matplotlib\n",
    "# APIs - we can't do anything about it so suppress them from view\n",
    "from matplotlib import MatplotlibDeprecationWarning\n",
    "warnings.simplefilter(\"ignore\", category=MatplotlibDeprecationWarning)\n",
    "\n",
    "print('Imports completed.') \n",
    "\n",
    "WIDGET_DEFAULTS = {'layout': widgets.Layout(width='95%'),\n",
    "                   'style': {'description_width': 'initial'}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Last Query to Clipboard\n",
    "This section adds an IPython magic command 'la'. The magic is used as follows:\n",
    "\n",
    "```%la [pythonvar|string]```\n",
    "\n",
    "If used with no arguments it will copy the last KQL query to clipboard and display a link to take you to the Sentinel/Log Analytics portal. If the argument is a variable the value of the variable is copied, otherwise the string is copied.\n",
    "\n",
    "When using the **%%la** cell magic form the entire cell is copied to the clipboard.\n",
    "\n",
    "The URL uses the current config settings for workspace and subscription.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# JS functions to hide/display code cells\n",
    "from IPython.display import HTML\n",
    "display(HTML(mas.util._TOGGLE_CODE_PREPARE_STR))\n",
    "\n",
    "_TOGGLE_CODE_STR = '''\n",
    "<form action=\"javascript:code_toggle()\">\n",
    "    <input type=\"submit\" id=\"toggleButton\" value=\"Show/Hide Code\">\n",
    "</form>\n",
    "'''\n",
    "\n",
    "_TOGGLE_CODE_PREPARE_STR = '''\n",
    "    <script>\n",
    "    function code_toggle() {\n",
    "        if ($('div.cell.code_cell.rendered.selected div.input').css('display')!='none'){\n",
    "            $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "        } else {\n",
    "            $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "        }\n",
    "    }\n",
    "    function hide_code() {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    }\n",
    "    function show_code() {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    </script>\n",
    "\n",
    "'''\n",
    "display(HTML(_TOGGLE_CODE_PREPARE_STR))\n",
    "\n",
    "def toggle_code():\n",
    "    \"\"\"Display a toggle button to hide/reveal code cell.\"\"\"\n",
    "    display(HTML(_TOGGLE_CODE_STR))\n",
    "\n",
    "def hide_code():\n",
    "    \"\"\"Display a toggle button to hide/reveal code cell.\"\"\"\n",
    "    display(HTML(_TOGGLE_CODE_STR))\n",
    "    display(HTML('<script>hide_code()</script>'))\n",
    "    \n",
    "def show_code():\n",
    "    \"\"\"Force code in cell to be visible.\"\"\"\n",
    "    display(HTML('<script>show_code()</script>'))\n",
    "    \n",
    "print('Code cell hiding initialized.') \n",
    "\n",
    "# Create %la magic\n",
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)\n",
    "from collections import OrderedDict\n",
    "\n",
    "def copy_to_clipboard(copy_text):\n",
    "    pd.DataFrame([copy_text]).to_clipboard(index=False,header=False)\n",
    "\n",
    "\n",
    "LA_URL=('https://ms.portal.azure.com/'\n",
    "        '?feature.showassettypes=Microsoft_Azure_Security_Insights_SecurityInsightsDashboard'\n",
    "        '#blade/Microsoft_Azure_Security_Insights/MainMenuBlade/7'\n",
    "        '/subscriptionId/{sub_id}'\n",
    "        '/resourceGroup/{res_group}'\n",
    "        '/workspaceName/{ws_name}')\n",
    "\n",
    "LA_URL_BARE=('https://ms.portal.azure.com/'\n",
    "             '?feature.showassettypes=Microsoft_Azure_Security_Insights_SecurityInsightsDashboard'\n",
    "             '#blade/Microsoft_Azure_Security_Insights/MainMenuBlade/')\n",
    "@register_line_cell_magic\n",
    "def la(line, cell=None):\n",
    "    KQL_MAGIC_RESULT = '_kql_raw_result_'\n",
    "    \n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    if not cell and not line or line.strip() == KQL_MAGIC_RESULT:\n",
    "        if KQL_MAGIC_RESULT in globals():\n",
    "            copy_to_clipboard(globals()[KQL_MAGIC_RESULT].query)\n",
    "            print('Last kql query copied to clipboard.')\n",
    "    elif line and cell is None:\n",
    "        if line in globals():\n",
    "            copy_to_clipboard(globals()[line])\n",
    "            print(f'Value of {line} copied to clipboard.')\n",
    "        elif line in locals():\n",
    "            copy_to_clipboard(locals()[line])\n",
    "            print(f'Value of {line} copied to clipboard.')\n",
    "        else:\n",
    "            copy_to_clipboard(line)\n",
    "            print(f'Copied to clipboard.')\n",
    "    else:\n",
    "        copy_to_clipboard(cell)\n",
    "        print(f'Copied to clipboard.')\n",
    "    \n",
    "    try:\n",
    "        ws_config = WorkspaceConfig(ws_config_file)\n",
    "        url = LA_URL.format(sub_id=ws_config['subscription_id'],\n",
    "                        res_group=ws_config['resource_group'], \n",
    "                        ws_name=ws_config['workspace_name'])\n",
    "    except:\n",
    "        url = LA_URL_BARE\n",
    "    return HTML(f'<a target=\"_new\" href=\"{url}\">Open Log Analytics</a>')\n",
    "del la\n",
    "print('%la magic registered')\n",
    "\n",
    "\n",
    "# Create an observation collector list\n",
    "from collections import namedtuple\n",
    "Observation = namedtuple('Observation', ['caption', 'description', 'item', 'link'])\n",
    "observation_list = OrderedDict()\n",
    "def display_observation(observation):\n",
    "    display(Markdown(f'### {observation.caption}'))\n",
    "    display(Markdown(observation.description))\n",
    "    display(Markdown(f'[Go to details](#{observation.link})'))\n",
    "    display(observation.item)\n",
    "\n",
    "def add_observation(observation):\n",
    "    observation_list[observation.caption] = observation\n",
    "\n",
    "print('Observation list initialized.')\n",
    "hide_code()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "_TOGGLE_CODE_STR = '''\n",
    "<form action=\"javascript:code_toggle()\">\n",
    "    <input type=\"submit\" id=\"toggleButton\" value=\"Show/Hide Code\">\n",
    "</form>\n",
    "'''\n",
    "\n",
    "_TOGGLE_CODE_PREPARE_STR = '''\n",
    "    <script>\n",
    "    function code_toggle() {\n",
    "        if ($('div.cell.code_cell.rendered.selected div.input').css('display')!='none'){\n",
    "            $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "        } else {\n",
    "            $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "        }\n",
    "    }\n",
    "    function hide_code() {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "    }\n",
    "    function show_code() {\n",
    "        $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "    }\n",
    "    </script>\n",
    "\n",
    "'''\n",
    "display(HTML(_TOGGLE_CODE_PREPARE_STR))\n",
    "\n",
    "\n",
    "def hide_code():\n",
    "    \"\"\"Display a toggle button to hide/reveal code cell.\"\"\"\n",
    "    display(HTML(_TOGGLE_CODE_STR))\n",
    "    display(HTML('<script>hide_code()</script>'))\n",
    "\n",
    "def show_code():\n",
    "    \"\"\"Display a toggle button to hide/reveal code cell.\"\"\"\n",
    "    display(HTML('<script>show_code()</script>'))\n",
    "hide_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove"
    ]
   },
   "source": [
    "### Get WorkspaceId\n",
    "To find your Workspace Id go to [Log Analytics](https://ms.portal.azure.com/#blade/HubsExtension/Resources/resourceType/Microsoft.OperationalInsights%2Fworkspaces). Look at the workspace properties to find the ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "todo"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from msticpy.nbtools.wsconfig import WorkspaceConfig\n",
    "ws_config_file = 'config.json'\n",
    "\n",
    "WORKSPACE_ID = None\n",
    "TENANT_ID = None\n",
    "try:\n",
    "    ws_config = WorkspaceConfig(ws_config_file)\n",
    "    display(Markdown(f'Read Workspace configuration from local config.json for workspace **{ws_config[\"workspace_name\"]}**'))\n",
    "    for cf_item in ['tenant_id', 'subscription_id', 'resource_group', 'workspace_id', 'workspace_name']:\n",
    "        display(Markdown(f'**{cf_item.upper()}**: {ws_config[cf_item]}'))\n",
    "                     \n",
    "    if ('cookiecutter' not in ws_config['workspace_id'] or\n",
    "            'cookiecutter' not in ws_config['tenant_id']):\n",
    "        WORKSPACE_ID = ws_config['workspace_id']\n",
    "        TENANT_ID = ws_config['tenant_id']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if not WORKSPACE_ID or not TENANT_ID:\n",
    "    display(Markdown('**Workspace configuration not found.**\\n\\n'\n",
    "                     'Please go to your Log Analytics workspace, copy the workspace ID'\n",
    "                     ' and/or tenant Id and paste here.<br> '\n",
    "                     'Or read the workspace_id from the config.json in your Azure Notebooks project.'))\n",
    "    ws_config = None\n",
    "    ws_id = mas.GetEnvironmentKey(env_var='WORKSPACE_ID',\n",
    "                              prompt='Please enter your Log Analytics Workspace Id:', auto_display=True)\n",
    "    ten_id = mas.GetEnvironmentKey(env_var='TENANT_ID',\n",
    "                              prompt='Please enter your Log Analytics Tenant Id:', auto_display=True) \n",
    "hide_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate to Log Analytics\n",
    "If you are using user/device authentication, run the following cell. \n",
    "- Click the 'Copy code to clipboard and authenticate' button.\n",
    "- This will pop up an Azure Active Directory authentication dialog (in a new tab or browser window). The device code will have been copied to the clipboard. \n",
    "- Select the text box and paste (Ctrl-V/Cmd-V) the copied value. \n",
    "- You should then be redirected to a user authentication page where you should authenticate with a user account that has permission to query your Log Analytics workspace.\n",
    "\n",
    "Use the following syntax if you are authenticating using an Azure Active Directory AppId and Secret:\n",
    "```\n",
    "%kql loganalytics://tenant(aad_tenant).workspace(WORKSPACE_ID).clientid(client_id).clientsecret(client_secret)\n",
    "```\n",
    "instead of\n",
    "```\n",
    "%kql loganalytics://code().workspace(WORKSPACE_ID)\n",
    "```\n",
    "\n",
    "Note: you may occasionally see a JavaScript error displayed at the end of the authentication - you can safely ignore this.<br>\n",
    "On successful authentication you should see a ```popup schema``` button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "todo"
    ]
   },
   "outputs": [],
   "source": [
    "TENANT_ID = '72f988bf-86f1-41af-91ab-2d7cd011db47'\n",
    "WORKSPACE_ID = '52b1ab41-869e-4138-9e40-2a4457f09bf0'\n",
    "if not WORKSPACE_ID or not TENANT_ID:\n",
    "    try:\n",
    "        WORKSPACE_ID = ws_id.value\n",
    "        TENANT_ID = ten_id.value\n",
    "    except NameError:\n",
    "        raise ValueError('No workspace or Tenant Id.')\n",
    "\n",
    "mas.kql.load_kql_magic()\n",
    "%kql loganalytics://code().tenant(TENANT_ID).workspace(WORKSPACE_ID)\n",
    "show_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%kql search * | summarize RowCount=count() by Type | project-rename Table=Type\n",
    "la_table_set = _kql_raw_result_.to_dataframe()\n",
    "table_index = la_table_set.set_index('Table')['RowCount'].to_dict()\n",
    "display(Markdown('Current data in workspace'))\n",
    "display(la_table_set.T)\n",
    "show_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='enteralertid'></a>[Contents](#toc)\n",
    "# Choose Alert to Investigate\n",
    "Pick an alert from a list of retrieved alerts.\n",
    "\n",
    "This section extracts the alert information and entities into a SecurityAlert object allowing us to query the properties more reliably. \n",
    "\n",
    "In particular, we use the alert to automatically provide parameters for queries and UI elements.\n",
    "Subsequent queries will use properties like the host name and derived properties such as the OS family (Linux or Windows) to adapt the query. Query time selectors like the one above will also default to an origin time that matches the alert selected.\n",
    "\n",
    "The alert view below shows all of the main properties of the alert plus the extended property dictionary (if any) and JSON representations of the Entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "todo"
    ]
   },
   "source": [
    "### Choose Timespan to look for alerts\n",
    "\n",
    "Specify a time range to search for alerts. One this is set run the following cell to retrieve any alerts in that time window.<br>\n",
    "You can change the time range and re-run the queries until you find the alerts that you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "show_code()\n",
    "alert_origin = datetime(2019, 2, 18)\n",
    "alert_q_times = mas.QueryTime(units='day', max_before=20, before=1, after=1, max_after=5, origin_time=alert_origin)\n",
    "alert_q_times.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select alert from list\n",
    "As you select an alert, the main properties will be shown below the list.<br>\n",
    "Use the filter box to narrow down your search to any substring in the AlertName."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Query for alerts\n",
    "alert_counts = qry.list_alerts_counts(provs=[alert_q_times])\n",
    "alert_list = qry.list_alerts(provs=[alert_q_times])\n",
    "print(len(alert_counts), ' distinct alert types')\n",
    "print(len(alert_list), ' distinct alerts')\n",
    "\n",
    "from msticpy.nbtools.entityschema import GeoLocation\n",
    "from msticpy.sectools.geoip import GeoLiteLookup\n",
    "iplocation = GeoLiteLookup()\n",
    "\n",
    "security_alert = None\n",
    "alert_ip_entities = []\n",
    "\n",
    "def show_full_alert(selected_alert):\n",
    "    global security_alert, alert_ip_entities\n",
    "    security_alert = mas.SecurityAlert(alert_select.selected_alert)\n",
    "    alert_ip_entities = get_alert_ips(security_alert)\n",
    "    mas.disp.display_alert(security_alert, show_entities=True)\n",
    "    \n",
    "def get_alert_ips(alert):\n",
    "    alert_ip_entities = [e for e in alert.entities if isinstance(e, mas.IpAddress)]\n",
    "    for ip_entity in alert_ip_entities:\n",
    "        if 'Location' not in ip_entity or not ip_entity.Location:\n",
    "            iplocation.lookup_ip(ip_entity=ip_entity)\n",
    "            \n",
    "    # Extract any additional IP addresses from text in the alert\n",
    "    ioc_extractor = sectools.IoCExtract()\n",
    "    alert_iocs = ioc_extractor.extract(src=str(alert))\n",
    "    addl_ip_addrs = alert_iocs.get('ipv4', [])\n",
    "    \n",
    "    if addl_ip_addrs:\n",
    "        _, ip_entities = iplocation.lookup_ip(ip_addr_list=addl_ip_addrs)\n",
    "\n",
    "        current_ips = [e.Address for e in alert_ip_entities]\n",
    "        for new_ip_entity in ip_entities:\n",
    "            if new_ip_entity.Address not in current_ips:\n",
    "                alert_ip_entities.append(new_ip_entity)\n",
    "    return alert_ip_entities\n",
    "\n",
    "hide_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the alert list\n",
    "show_code()\n",
    "alert_select = mas.AlertSelector(alerts=alert_list, action=show_full_alert)\n",
    "alert_select.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host from alert + we've extracted the IP address and looked its location\n",
    "show_code()\n",
    "print(security_alert.primary_host)\n",
    "\n",
    "print(alert_ip_entities)\n",
    "\n",
    "add_observation(Observation(caption='Initial Alert', \n",
    "                            description='Suspicious download alert',\n",
    "                            item=security_alert,\n",
    "                            link='enteralertid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See what is happening on this Linux host\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- We can see that it's a Linux host and can view the alert time.\n",
    "- Set the time boundaries to be a hour or two either side of the alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%config Kqlmagic.auto_dataframe=False\n",
    "host1_q_times = mas.QueryTime(label='Set time bounds for alert host - at least 1hr either side of the alert',\n",
    "                           units='hour', max_before=48, before=2, after=1, \n",
    "                           max_after=24, origin_time=security_alert.StartTimeUtc)\n",
    "host1_q_times.display()\n",
    "show_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass the host Id and time boundaries to query Auditd events from host\n",
    "But... Data is in raw Auditd format so we need to decode it and assemble into single events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os import path\n",
    "\n",
    "# Query to Retrieve Linux auditd events\n",
    "# Collapsing messages with the same mssg_id into a single row\n",
    "linux_events = r'''\n",
    "AuditLog_CL\n",
    "| where Computer has '{hostname}'\n",
    "| where TimeGenerated >= datetime({start})\n",
    "| where TimeGenerated <= datetime({end})\n",
    "| extend mssg_parts = extract_all(@\"type=(?P<type>[^\\s]+)\\s+msg=audit\\((?P<mssg_id>[^)]+)\\):\\s+(?P<mssg>[^\\r]+)\\r?\", dynamic(['type', 'mssg_id', 'mssg']), RawData)\n",
    "| extend mssg_type = tostring(mssg_parts[0][0]), mssg_id = tostring(mssg_parts[0][1])\n",
    "| project TenantId, TimeGenerated, Computer, mssg_type, mssg_id, mssg_parts\n",
    "| extend mssg_content = split(mssg_parts[0][2],' ')\n",
    "| extend typed_mssg = pack(mssg_type, mssg_content)\n",
    "| summarize AuditdMessage = makelist(typed_mssg) by TenantId, TimeGenerated, Computer, mssg_id\n",
    "'''.format(start=host1_q_times.start, end=host1_q_times.end,\n",
    "           hostname=security_alert.hostname)\n",
    "\n",
    "_PICKLE_FILE = 'lx_auditd.cache'\n",
    "linux_events_all = None\n",
    "# if path.exists(_PICKLE_FILE):\n",
    "#     with open(_PICKLE_FILE, 'rb') as cache_file:\n",
    "#         linux_events_all = pickle.load(cache_file)\n",
    "\n",
    "if linux_events_all is None:\n",
    "    print('getting data...')\n",
    "    %kql -query linux_events\n",
    "    linux_events_df = _kql_raw_result_.to_dataframe()\n",
    "    print(f'{len(linux_events_df)} raw auditd mssgs downloaded')\n",
    "\n",
    "\n",
    "# Auditd Transform code\n",
    "import codecs\n",
    "from datetime import datetime\n",
    "encoded_params = {'EXECVE': {'a0', 'a1', 'a2', 'a3', 'arch'},\n",
    "                  'PROCTITLE': {'proctitle'},\n",
    "                  'USER_CMD': {'cmd'}}\n",
    "\n",
    "def unpack_auditd(audit_str):\n",
    "    \"\"\"\n",
    "    Unpack an auditd combined message into its component parts.\n",
    "    \n",
    "    Arguments:\n",
    "        audit_str: str -- list of messages sharing same mssg_id\n",
    "    \n",
    "    Returns:\n",
    "        dict{str, dict{str, str}}\n",
    "        \n",
    "    Description:\n",
    "        Input structure is:\n",
    "            list[mssgs]\n",
    "            mssgs:- dict[mssg_type, mssg_fields]\n",
    "            mssg_type:- str (SYSCALL, LOGIN, etc.)\n",
    "            mssg_fields:- list[mssg_field]\n",
    "            mssg_field:- name:value\n",
    "            name:- str (name of field)\n",
    "            value:- str (value of field)\n",
    "    \"\"\"\n",
    "    event_dict = {}\n",
    "    \n",
    "    for record in audit_str:\n",
    "        # each record is a diction of mssg_type/mssg_fields\n",
    "        # each mssg_fields is a list of k/v pairs delimited\n",
    "        # by '='\n",
    "        for rec_key, rec_val in record.items():\n",
    "            rec_dict = {}\n",
    "            encoded_fields_map = encoded_params.get(rec_key, None)\n",
    "            for rec_item in rec_val:\n",
    "                rec_split = rec_item.split('=', maxsplit=1)\n",
    "                if len(rec_split) == 1:\n",
    "                    rec_dict[rec_split[0]] = None\n",
    "                    continue\n",
    "                # For unencoded fields just extract the string\n",
    "                if (not encoded_fields_map or rec_split[1].startswith('\"') or\n",
    "                        rec_split[0] not in encoded_fields_map):\n",
    "                    field_value = rec_split[1].strip('\\\"')\n",
    "                else:\n",
    "                    # otherwise try to decode\n",
    "                    try:\n",
    "                        field_value = codecs.decode(rec_split[1], 'hex').decode('utf-8')\n",
    "                    except:\n",
    "                        field_value = rec_split[1]\n",
    "                        print(rec_val)\n",
    "                        print('ERR:', rec_key, rec_split[0], rec_split[1], type(rec_split[1]))\n",
    "                # add the extract field/value to rec_dict\n",
    "                rec_dict[rec_split[0]] = field_value\n",
    "            # add all fields in the record to the event_dict\n",
    "            event_dict[rec_key] = rec_dict\n",
    "        \n",
    "    return event_dict\n",
    "\n",
    "# Const dictionaries used for unpacking/transforming Auditd records\n",
    "USER_START = {'pid': 'int', 'uid': 'int', 'auid': 'int', \n",
    "              'ses': 'int', 'msg': None, 'acct': None, 'exe': None, \n",
    "              'hostname': None, 'addr': None, 'terminal': None, \n",
    "              'res': None}\n",
    "FIELD_DEFS = {'SYSCALL': {'success': None, 'ppid': 'int', 'pid': 'int', \n",
    "                          'auid': 'int', 'uid': 'int', 'gid': 'int',\n",
    "                          'euid': 'int', 'egid': 'int', 'ses': 'int',\n",
    "                          'exe': None, 'com': None},\n",
    "              'CWD': {'cwd': None},\n",
    "              'PROCTITLE': {'proctitle': None},\n",
    "              'LOGIN': {'pid': 'int', 'uid': 'int', 'tty': None, 'old-ses': 'int', \n",
    "                        'ses': 'int', 'res': None},\n",
    "              'EXECVE': {'argc': 'int', 'a0': None, 'a1': None, 'a2': None},\n",
    "              'USER_START': USER_START,\n",
    "              'USER_END': USER_START,\n",
    "              'CRED_DISP': USER_START,\n",
    "              'USER_ACCT': USER_START,\n",
    "              'CRED_ACQ': USER_START,\n",
    "              'USER_CMD': {'pid': 'int', 'uid': 'int', 'auid': 'int', \n",
    "                           'ses': 'int', 'msg': None, 'cmd': None,\n",
    "                           'terminal': None, 'res': None},\n",
    "             }\n",
    "\n",
    "def extract_event(message_dict):\n",
    "    \"\"\"\n",
    "    Extract event from unpacked auditd message.\n",
    "    \n",
    "    Arguments:\n",
    "        message_dict: dict{str, dict{str, str}} --\n",
    "            this is the output format from unpack_auditd\n",
    "    \n",
    "    Returns:\n",
    "        tuple{str, dict}\n",
    "        \n",
    "    Description:\n",
    "        Assembles multiple mssg value sets into single dict.\n",
    "        Converts str to int if required.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Treat SYSCALL messages specially since these are our\n",
    "    # process start events\n",
    "    if 'SYSCALL' in message_dict:\n",
    "        proc_create_dict = {}\n",
    "        # Currently ignoring PATH messages\n",
    "        for mssg_type in ['SYSCALL', 'CWD', 'EXECVE', 'PROCTITLE']:\n",
    "            if (not mssg_type in message_dict or\n",
    "                    not mssg_type in FIELD_DEFS) :\n",
    "                continue\n",
    "            # if the field is flagged to be converted\n",
    "            for fieldname, conv in FIELD_DEFS[mssg_type].items():\n",
    "                value = message_dict[mssg_type].get(fieldname, None)\n",
    "                if not value:\n",
    "                    continue\n",
    "                if conv:\n",
    "                    if conv == 'int':\n",
    "                        value = int(value)\n",
    "                        # hex FFFF FFFF == -1 signed\n",
    "                        if value == 4294967295:\n",
    "                            value = -1\n",
    "                proc_create_dict[fieldname] = value\n",
    "            if mssg_type == 'EXECVE':\n",
    "                # reassemble the args in EXECVE to the original arg string\n",
    "                args = int(proc_create_dict.get('argc', 1))\n",
    "                arg_strs = []\n",
    "                for arg_idx in range(0, args):\n",
    "                    arg_strs.append(proc_create_dict.get(f'a{arg_idx}', ''))\n",
    "                    \n",
    "                proc_create_dict['cmdline'] = ' '.join(arg_strs)\n",
    "        return 'SYSCALL', proc_create_dict\n",
    "    else:\n",
    "        event_dict = {}                                            \n",
    "        for mssg_type, mssg in message_dict.items():\n",
    "            if mssg_type in FIELD_DEFS:\n",
    "                for fieldname, conv in FIELD_DEFS[mssg_type].items():\n",
    "                    value = message_dict[mssg_type].get(fieldname, None)\n",
    "                    if conv:\n",
    "                        if conv == 'int':\n",
    "                            value = int(value)\n",
    "                            if value == 4294967295:\n",
    "                                value = -1\n",
    "                    event_dict[fieldname] = value\n",
    "            else:\n",
    "                \n",
    "                event_dict.update(message_dict[mssg_type])\n",
    "        return list(message_dict.keys())[0], event_dict\n",
    "\n",
    "    \n",
    "def move_cols_to_front(df, column_count):\n",
    "    \"\"\"Reorder columms to put the last column count cols to front.\"\"\"\n",
    "    return df[list(df.columns[-column_count:]) + list(df.columns[:-column_count])]\n",
    "\n",
    "\n",
    "def extract_events_to_df(data, event_type=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Extract raw auditd to DataFrame.\n",
    "    \n",
    "    Arguments:\n",
    "        data {pd.DataFrame} - input dataframe with auditd\n",
    "            message in AuditdMessage string field\n",
    "        event_type {str} -- if specified only auditd messages\n",
    "            of this type will be processed.\n",
    "        verbose {bool} -- give verbose reporting of progress\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        start_time = datetime.utcnow()\n",
    "        print(f'Unpacking auditd messages for {len(data)} events...')\n",
    "    tmp_df = (data.apply(lambda x: extract_event(unpack_auditd(x.AuditdMessage)), \n",
    "                         axis=1, result_type='expand')\n",
    "                  .rename(columns={0: 'EventType', \n",
    "                                   1: 'EventData'})\n",
    "                  )\n",
    "    # if only one type of event is requested, just match\n",
    "    # on that type\n",
    "    if event_type:\n",
    "        tmp_df = tmp_df[tmp_df['EventType'] == event_type]\n",
    "        if verbose:\n",
    "            print(f'Event subset = ', event_type, ' (events: {len(tmp_df)})')\n",
    "    \n",
    "    # build a dataframe by from the event dictionary\n",
    "    if verbose:\n",
    "        print('Building output dataframe...')\n",
    "    \n",
    "    # line 1 - return DF of series created from eventdata dict\n",
    "    tmp_df = (tmp_df.apply(lambda x: pd.Series(x.EventData), axis=1)\n",
    "              # merge with the same input dataframe to retain event row\n",
    "              # metadata (Computer, TenantId, etc.)\n",
    "              .merge(tmp_df[['EventType']], left_index=True, right_index=True)\n",
    "              .merge(data.drop(['AuditdMessage'], axis=1), \n",
    "                 how='inner', left_index=True, right_index=True)\n",
    "              .dropna(axis=1, how='all'))\n",
    "    \n",
    "    if verbose:\n",
    "        print('Fixing timestamps...')\n",
    "    # extract real timestamp from mssg_id\n",
    "    tmp_df['TimeStamp'] = (tmp_df.apply(lambda x:\n",
    "                                        datetime.utcfromtimestamp(float(x.mssg_id.split(':')[0])),\n",
    "                                        axis=1))\n",
    "    tmp_df = (tmp_df.drop(['TimeGenerated'], axis=1)\n",
    "                    .rename(columns={'TimeStamp': 'TimeGenerated'})\n",
    "                    .pipe(move_cols_to_front, column_count=5))\n",
    "    if verbose:\n",
    "        print(f'Complete. {len(tmp_df)} output rows', end=' ')\n",
    "        delta = datetime.utcnow() - start_time\n",
    "        print(f'time: {delta.seconds + delta.microseconds/1_000_000} sec')\n",
    "        \n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "def get_event_subset(data, event_type):\n",
    "    return (data[data['EventType'] == event_type]\n",
    "             .dropna(axis=1, how='all')\n",
    "             .infer_objects())\n",
    "\n",
    "hide_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_code()\n",
    "linux_events_all = extract_events_to_df(linux_events_df, verbose=True)\n",
    "\n",
    "with open(_PICKLE_FILE, 'ab') as cache_file:\n",
    "    print('Writing cache file')\n",
    "    pickle.dump(linux_events_all, cache_file)\n",
    "        \n",
    "print(f'{len(linux_events_all)} events read.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linux_login_ips'></a>[Contents](#toc)\n",
    "## Get Any External Logins (that have an IP Address Recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hide_code()\n",
    "\n",
    "print('Extracting logon and process events')\n",
    "\n",
    "# extract \"process_create\" and login events from our\n",
    "# extracted Auditd\n",
    "lx_proc_create = get_event_subset(linux_events_all,'SYSCALL')\n",
    "print(f'{len(lx_proc_create)} Process Create Events')\n",
    "\n",
    "# extract LOGIN and merge with CRED_ACQ - so that we can\n",
    "# map process events with the correct logon session\n",
    "lx_login = (get_event_subset(linux_events_all, 'LOGIN')\n",
    "        .merge(get_event_subset(linux_events_all, 'CRED_ACQ'), \n",
    "               how='inner',\n",
    "               left_on=['old-ses', 'pid', 'uid'], \n",
    "               right_on=['ses', 'pid', 'uid'],\n",
    "               suffixes=('', '_cred')).drop(['old-ses','TenantId_cred', \n",
    "                                             'Computer_cred'], axis=1)\n",
    "        .dropna(axis=1, how='all'))\n",
    "print(f'{len(lx_login)} Login Events')\n",
    "\n",
    "\n",
    "display(Markdown('### Logons with external IPs'))                   \n",
    "# Show logins where the address field has a value\n",
    "logins_with_ips = (lx_login[lx_login['addr'] != '?']\n",
    "                   [['Computer', 'TimeGenerated','pid', 'ses', \n",
    "                     'acct', 'addr', 'exe', 'hostname', 'msg',\n",
    "                     'res_cred', 'ses_cred', 'terminal']])\n",
    "\n",
    "# If we have any - add these to the alert_ip_entities that \n",
    "# we're tracking\n",
    "if len(logins_with_ips) > 0:\n",
    "    display(logins_with_ips)\n",
    "    \n",
    "    # add the source IPs if not alert \n",
    "    new_ips = logins_with_ips['addr'].drop_duplicates().tolist()\n",
    "    current_ips = [e.Address for e in alert_ip_entities]\n",
    "    for new_ip in new_ips:\n",
    "        if new_ip not in current_ips:\n",
    "            ip_entity = mas.IpAddress(Address=new_ip)\n",
    "            iplocation.lookup_ip(ip_entity=ip_entity)\n",
    "            alert_ip_entities.append(ip_entity)\n",
    "    \n",
    "\n",
    "# Mapping to Windows-like event to work more easily with clustering\n",
    "lx_to_proc_create = {'acct': 'SubjectUserName',\n",
    "                     'uid': 'SubjectUserSid',\n",
    "                     'user': 'SubjectUserName',\n",
    "                     'ses': 'SubjectLogonId',\n",
    "                     'pid': 'NewProcessId',\n",
    "                     'exe': 'NewProcessName',\n",
    "                     'ppid': 'ProcessId',\n",
    "                     'cmdline': 'CommandLine',}\n",
    "\n",
    "proc_create_to_lx = {'SubjectUserName': 'acct',\n",
    "                     'SubjectUserSid': 'uid',\n",
    "                     'SubjectUserName': 'user',\n",
    "                     'SubjectLogonId': 'ses',\n",
    "                     'NewProcessId': 'pid',\n",
    "                     'NewProcessName': 'exe',\n",
    "                     'ProcessId': 'ppid',\n",
    "                     'CommandLine': 'cmdline',}\n",
    "\n",
    "lx_to_logon = {'acct': 'SubjectUserName',\n",
    "               'auid': 'SubjectUserSid',\n",
    "               'user': 'TargetUserName',\n",
    "               'uid': 'TargetUserSid',\n",
    "               'ses': 'TargetLogonId',\n",
    "               'exe': 'LogonProcessName',\n",
    "               'terminal': 'LogonType',\n",
    "               'msg': 'AuthenticationPackageName',\n",
    "               'res': 'Status',\n",
    "               'addr': 'IpAddress',\n",
    "               'hostname': 'WorkstationName',}\n",
    "\n",
    "logon_to_lx = {'SubjectUserName': 'acct',\n",
    "               'SubjectUserSid': 'auid',\n",
    "               'TargetUserName': 'user',\n",
    "               'TargetUserSid': 'uid',\n",
    "               'TargetLogonId': 'ses',\n",
    "               'LogonProcessName': 'exe',\n",
    "               'LogonType': 'terminal',\n",
    "               'AuthenticationPackageName': 'msg',\n",
    "               'Status': 'res',\n",
    "               'IpAddress': 'addr',\n",
    "               'WorkstationName': 'hostname',}\n",
    "\n",
    "lx_proc_create_trans = lx_proc_create.rename(columns=lx_to_proc_create)\n",
    "lx_login_trans = lx_login.rename(columns=lx_to_logon)\n",
    "\n",
    "print('analyzing data...')\n",
    "from msticpy.sectools.eventcluster import dbcluster_events, add_process_features\n",
    "\n",
    "# This section takes the input process events and clusters them\n",
    "# to try to collapse common repetitive events\n",
    "feature_procs_h1 = add_process_features(input_frame=lx_proc_create_trans,\n",
    "                                        path_separator=security_alert.path_separator)\n",
    "\n",
    "\n",
    "# you might need to play around with the max_cluster_distance parameter.\n",
    "# decreasing this gives more clusters.\n",
    "(clus_events, dbcluster, x_data) = dbcluster_events(data=feature_procs_h1,\n",
    "                                                    cluster_columns=['commandlineTokensFull', \n",
    "                                                                     'pathScore',\n",
    "                                                                    'SubjectUserSid'],\n",
    "                                                    time_column='TimeGenerated',\n",
    "                                                    max_cluster_distance=0.0001)\n",
    "print('Number of input events:', len(feature_procs_h1))\n",
    "print('Number of clustered events:', len(clus_events))\n",
    "(clus_events.sort_values('TimeGenerated')[['TimeGenerated', 'LastEventTime',\n",
    "                                           'NewProcessName', 'CommandLine', \n",
    "                                           'ClusterSize', 'commandlineTokensFull',\n",
    "                                           'SubjectLogonId', 'SubjectUserSid',\n",
    "                                           'pathScore', 'isSystemSession']]\n",
    "    .sort_values('ClusterSize', ascending=True));\n",
    "\n",
    "\n",
    "# Widet callback to display process set for the selected logon\n",
    "def view(x=''):\n",
    "    procs = (clus_events[clus_events['SubjectLogonId']==x]\n",
    "            [['TimeGenerated', 'NewProcessName','CommandLine', \n",
    "              'NewProcessId', 'SubjectUserSid', 'cwd', 'ClusterSize', 'SubjectLogonId']])\n",
    "    display(Markdown(f'{len(procs)} process events'))\n",
    "    display(procs.query('SubjectUserSid != 0'))\n",
    "    \n",
    "\n",
    "display(Markdown('### What is happening in these sessions?'))\n",
    "display(Markdown('Click on a session Id to view the processes that ran.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_code()\n",
    "items = logins_with_ips['ses'].unique().tolist()\n",
    "w = widgets.Select(options=items, description='Select Session to view', **WIDGET_DEFAULTS)\n",
    "interactive(view, x=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "One session looks innocuous but in the other...we can see that the attacker is:\n",
    "- Retrieving host information including \n",
    "  - network information\n",
    "  - contents of /etc/passwd and mail.\n",
    "- Looks like a shell script is being downloaded and set as executable. \n",
    "- Finally, looks like something is happening with the cron scheduler - this is just listing crontab but likely this is a precursor to installing a persistent presence on the machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='alert_host_net'></a>[Contents](#toc)\n",
    "# Host Network Data\n",
    "### Now look at the host's network activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%config Kqlmagic.auto_dataframe=False\n",
    "host1_q_times = mas.QueryTime(label='Set time bounds for alert host - at least 1hr either side of the alert',\n",
    "                           units='hour', max_before=48, before=2, after=1, \n",
    "                           max_after=24, origin_time=security_alert.StartTimeUtc)\n",
    "host1_q_times.display()\n",
    "show_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from numbers import Number\n",
    "import warnings\n",
    "\n",
    "hide_code()\n",
    "\n",
    "# Get the host from our alert\n",
    "host_entities = [e for e in security_alert.entities if isinstance(e, mas.Host)]\n",
    "if len(host_entities) == 1:\n",
    "    inv_host_entity = host_entities[0]\n",
    "    host_name = inv_host_entity.HostName\n",
    "    resource = inv_host_entity.AzureID\n",
    "else:\n",
    "    host_name = None\n",
    "    inv_host_entity = None\n",
    "    print('Error: Could not determine host entity from alert.')\n",
    "\n",
    "# Folium map functions - now migrated to msticpy.sectools.geoip module\n",
    "def create_ip_map():\n",
    "    folium_map = folium.Map(zoom_start=7, tiles=None, width='100%', height='100%')\n",
    "    folium_map.add_tile_layer(name='IPEvents')\n",
    "    return folium_map\n",
    "\n",
    "def add_ip_cluster(folium_map, ip_entities, alert=None, **icon_props):\n",
    "    if not folium_map:\n",
    "        folium_map = create_ip_map()\n",
    "    \n",
    "    for ip_entity in ip_entities:\n",
    "        if not (isinstance(ip_entity.Location.Latitude, Number) and\n",
    "                isinstance(ip_entity.Location.Longitude, Number)):\n",
    "            warnings.warn(\"Invalid location information for IP: \" + ip_entity.Address,\n",
    "                          RuntimeWarning)\n",
    "            continue\n",
    "        loc_props = ', '.join([f'{key}={val}' for key, val in \n",
    "                               ip_entity.Location.properties.items() if val])\n",
    "        popup_text = \"{loc_props}<br>{IP}\".format(IP=ip_entity.Address,\n",
    "                                                  loc_props=loc_props)\n",
    "        tooltip_text = '{City}, {CountryName}'.format(**ip_entity.Location.properties)\n",
    "        if alert:\n",
    "            popup_text = f'{popup_text}<br>{alert.AlertName}'\n",
    "        if ip_entity.AdditionalData:\n",
    "            addl_props = ', '.join([f'{key}={val}' for key, val in \n",
    "                                    ip_entity.AdditionalData.items() if val])\n",
    "            popup_text = f'{popup_text}<br>{addl_props}'\n",
    "            tooltip_text = f'{tooltip_text}, {addl_props}'\n",
    "        marker = folium.Marker(\n",
    "            location = [ip_entity.Location.Latitude, ip_entity.Location.Longitude],\n",
    "            popup=popup_text,\n",
    "            tooltip=tooltip_text,\n",
    "            icon=folium.Icon(**icon_props)\n",
    "        )\n",
    "        marker.add_to(folium_map)\n",
    "\n",
    "    return folium_map\n",
    "\n",
    "# We don't currently know the ip addresses of our alert host\n",
    "# so try to get them by looking at OMS heartbeat and Azure NSG logs\n",
    "print('Looking for IP addresses of ', host_name)\n",
    "aznet_query = '''\n",
    "AzureNetworkAnalytics_CL \n",
    "| where VirtualMachine_s has \\'{host}\\'\n",
    "| where ResourceType == 'NetworkInterface'\n",
    "| top 1 by TimeGenerated desc\n",
    "| project PrivateIPAddresses = PrivateIPAddresses_s, \n",
    "    PublicIPAddresses = PublicIPAddresses_s\n",
    "'''.format(host=host_name)\n",
    "%kql -query aznet_query\n",
    "az_net_df = _kql_raw_result_.to_dataframe()\n",
    "\n",
    "\n",
    "oms_heartbeat_query = '''\n",
    "Heartbeat\n",
    "| where Computer has \\'{host}\\'\n",
    "| top 1 by TimeGenerated desc nulls last\n",
    "| project ComputerIP, OSType, OSMajorVersion, OSMinorVersion, ResourceId, RemoteIPCountry, \n",
    "RemoteIPLatitude, RemoteIPLongitude, SourceComputerId\n",
    "'''.format(host=host_name)\n",
    "%kql -query oms_heartbeat_query\n",
    "oms_heartbeat_df = _kql_raw_result_.to_dataframe()\n",
    "\n",
    "\n",
    "print('getting data...')\n",
    "# Get the host entity and add this IP and system info to the \n",
    "try:\n",
    "    if not inv_host_entity:\n",
    "        inv_host_entity = mas.Host()\n",
    "        inv_host_entity.HostName = host_name\n",
    "except NameError:\n",
    "    inv_host_entity = mas.Host()\n",
    "    inv_host_entity.HostName = host_name\n",
    "\n",
    "def convert_to_ip_entities(ip_str):\n",
    "    ip_entities = []\n",
    "    if ip_str:\n",
    "        if ',' in ip_str:\n",
    "            addrs = ip_str.split(',')\n",
    "        elif ' ' in ip_str:\n",
    "            addrs = ip_str.split(' ')\n",
    "        else:\n",
    "            addrs = [ip_str]\n",
    "        for addr in addrs:\n",
    "            ip_entity = mas.IpAddress()\n",
    "            ip_entity.Address = addr.strip()\n",
    "            iplocation.lookup_ip(ip_entity=ip_entity)\n",
    "            ip_entities.append(ip_entity)\n",
    "            print(addr)\n",
    "    return ip_entities\n",
    "\n",
    "# Add this information to our inv_host_entity\n",
    "if len(az_net_df) == 1:\n",
    "    priv_addr_str = az_net_df['PrivateIPAddresses'].loc[0]\n",
    "    inv_host_entity.properties['private_ips'] = convert_to_ip_entities(priv_addr_str)\n",
    "\n",
    "    pub_addr_str = az_net_df['PublicIPAddresses'].loc[0]\n",
    "    inv_host_entity.properties['public_ips'] = convert_to_ip_entities(pub_addr_str)\n",
    "\n",
    "retrieved_address = [ip.Address for ip in inv_host_entity.properties['public_ips']]\n",
    "if len(oms_heartbeat_df) == 1:\n",
    "    if oms_heartbeat_df['ComputerIP'].loc[0]:\n",
    "        oms_address = oms_heartbeat_df['ComputerIP'].loc[0]\n",
    "        if oms_address not in retrieved_address:\n",
    "            ip_entity = mas.IpAddress()\n",
    "            ip_entity.Address = oms_address\n",
    "            iplocation.lookup_ip(ip_entity=ip_entity)\n",
    "            inv_host_entity.properties['public_ips'].append(ip_entity)\n",
    "        \n",
    "    inv_host_entity.OSFamily = oms_heartbeat_df['OSType'].loc[0]\n",
    "    inv_host_entity.AdditionalData['OSMajorVersion'] = oms_heartbeat_df['OSMajorVersion'].loc[0]\n",
    "    inv_host_entity.AdditionalData['OSMinorVersion'] = oms_heartbeat_df['OSMinorVersion'].loc[0]\n",
    "    inv_host_entity.AdditionalData['SourceComputerId'] = oms_heartbeat_df['SourceComputerId'].loc[0]\n",
    "\n",
    "\n",
    "# Having (hopefully) got the IP addresses of our alert host\n",
    "# we can find all of the flows to/from this host\n",
    "\n",
    "# Azure Network Analytics Base Query\n",
    "az_net_analytics_query =r'''\n",
    "AzureNetworkAnalytics_CL \n",
    "| where SubType_s == 'FlowLog'\n",
    "| where FlowStartTime_t >= datetime({start})\n",
    "| where FlowEndTime_t <= datetime({end})\n",
    "| project TenantId, TimeGenerated, \n",
    "    FlowStartTime = FlowStartTime_t, \n",
    "    FlowEndTime = FlowEndTime_t, \n",
    "    FlowIntervalEndTime = FlowIntervalEndTime_t, \n",
    "    FlowType = FlowType_s,\n",
    "    ResourceGroup = split(VM_s, '/')[0],\n",
    "    VMName = split(VM_s, '/')[1],\n",
    "    VMIPAddress = VMIP_s, \n",
    "    PublicIPs = extractall(@\"([\\d\\.]+)[|\\d]+\", dynamic([1]), PublicIPs_s),\n",
    "    SrcIP = SrcIP_s,\n",
    "    DestIP = DestIP_s,\n",
    "    ExtIP = iif(FlowDirection_s == 'I', SrcIP_s, DestIP_s),\n",
    "    L4Protocol = L4Protocol_s, \n",
    "    L7Protocol = L7Protocol_s, \n",
    "    DestPort = DestPort_d, \n",
    "    FlowDirection = FlowDirection_s,\n",
    "    AllowedOutFlows = AllowedOutFlows_d, \n",
    "    AllowedInFlows = AllowedInFlows_d,\n",
    "    DeniedInFlows = DeniedInFlows_d, \n",
    "    DeniedOutFlows = DeniedOutFlows_d,\n",
    "    RemoteRegion = AzureRegion_s,\n",
    "    VMRegion = Region_s\n",
    "| extend AllExtIPs = iif(isempty(PublicIPs), pack_array(ExtIP), \n",
    "                         iif(isempty(ExtIP), PublicIPs, array_concat(PublicIPs, pack_array(ExtIP)))\n",
    "                         )\n",
    "| project-away ExtIP\n",
    "| mvexpand AllExtIPs\n",
    "{where_clause}\n",
    "'''\n",
    "\n",
    "start = security_alert.StartTimeUtc - timedelta(seconds=(20 * 60 * 60))\n",
    "end = security_alert.StartTimeUtc + timedelta(seconds=(5 * 60 * 60))\n",
    "\n",
    "\n",
    "### Flow Time and Protocol Distribution\n",
    "# build the query - we need to assemble our host IPs into a list\n",
    "# that Kql will understand as part of an IN expression\n",
    "all_alert_host_ips = inv_host_entity.private_ips + inv_host_entity.public_ips\n",
    "host_ips = {'\\'{}\\''.format(i.Address) for i in all_alert_host_ips}\n",
    "alert_host_ip_list = ','.join(host_ips)\n",
    "\n",
    "az_ip_where = f'''\n",
    "| where (VMIPAddress in ({alert_host_ip_list}) \n",
    "        or SrcIP in ({alert_host_ip_list}) \n",
    "        or DestIP in ({alert_host_ip_list}) \n",
    "        ) and \n",
    "    (AllowedOutFlows > 0 or AllowedInFlows > 0)'''\n",
    "\n",
    "print('Reading network flow data.')\n",
    "print('getting data...', end=' ')\n",
    "az_net_query_byip = az_net_analytics_query.format(where_clause=az_ip_where,\n",
    "                                                  start=start,\n",
    "                                                  end=end)\n",
    "\n",
    "net_default_cols = ['FlowStartTime', 'FlowEndTime', 'VMName', 'VMIPAddress', \n",
    "                'PublicIPs', 'SrcIP', 'DestIP', 'L4Protocol', 'L7Protocol',\n",
    "                'DestPort', 'FlowDirection', 'AllowedOutFlows', \n",
    "                'AllowedInFlows']\n",
    "\n",
    "%kql -query az_net_query_byip\n",
    "az_net_comms_df = _kql_raw_result_.to_dataframe()\n",
    "print('done')\n",
    "\n",
    "# Turn these IP addresses into IP entities and add location\n",
    "# information\n",
    "ip_locs_in = set()\n",
    "ip_locs_out = set()\n",
    "for _, row in az_net_comms_df.iterrows():\n",
    "    ip = row.AllExtIPs\n",
    "\n",
    "    if ip in ip_locs_in or ip in ip_locs_out or not ip:\n",
    "        continue\n",
    "    ip_entity = mas.IpAddress(Address=ip)\n",
    "    iplocation.lookup_ip(ip_entity=ip_entity)\n",
    "    if not ip_entity.Location:\n",
    "        continue\n",
    "    ip_entity.AdditionalData['protocol'] = row.L7Protocol\n",
    "    if row.FlowDirection == 'I':\n",
    "        ip_locs_in.add(ip_entity)\n",
    "    else:\n",
    "        ip_locs_out.add(ip_entity)\n",
    "\n",
    "# Build the GeoIP Map\n",
    "flow_map = create_ip_map()\n",
    "\n",
    "icon_props = {'color': 'green'}\n",
    "flow_map = add_ip_cluster(folium_map=flow_map,\n",
    "                            ip_entities=inv_host_entity.public_ips,\n",
    "                            **icon_props)\n",
    "icon_props = {'color': 'blue'}\n",
    "flow_map = add_ip_cluster(folium_map=flow_map,\n",
    "                            ip_entities=ip_locs_out,\n",
    "                            **icon_props)\n",
    "icon_props = {'color': 'purple'}\n",
    "flow_map = add_ip_cluster(folium_map=flow_map,\n",
    "                            ip_entities=ip_locs_in,\n",
    "                            **icon_props)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_code()\n",
    "display(Markdown('### Flow Time and Protocol Distribution'))\n",
    "nbdisp.display_timeline(data=az_net_comms_df.query('AllowedOutFlows > 0'),\n",
    "                         overlay_data=az_net_comms_df.query('AllowedInFlows > 0'),\n",
    "                         alert=security_alert, title='Network Flows (out=blue, in=green)',\n",
    "                         time_column='FlowStartTime',\n",
    "                         source_columns=['FlowType', 'AllExtIPs', 'L7Protocol', 'FlowDirection'],\n",
    "                         height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "show_code()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    az_net_comms_df['TotalAllowedFlows'] = az_net_comms_df['AllowedOutFlows'] + az_net_comms_df['AllowedInFlows']\n",
    "    sns.catplot(x=\"L7Protocol\", y=\"TotalAllowedFlows\", col=\"FlowDirection\", data=az_net_comms_df)\n",
    "    sns.relplot(x=\"FlowStartTime\", y=\"TotalAllowedFlows\", \n",
    "                col=\"FlowDirection\", kind=\"line\", \n",
    "                hue=\"L7Protocol\", data=az_net_comms_df).set_xticklabels(rotation=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_code()\n",
    "display(HTML('<h3>External IP Addresses communicating with host</h3>'))\n",
    "display(HTML('Numbered circles indicate multiple items - click to expand'))\n",
    "display(HTML('Location markers: Blue = outbound, Purple = inbound, Green = Host'))\n",
    "display(flow_map)\n",
    "display(Markdown('<font color=\"red\">Warning: the folium mapping library '\n",
    "                 'does not display correctly in some browsers.</font> '\n",
    "                 'If you see a blank image please retry with a different browser.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- We can the relatively rare ssh inbound traffic and view the connections on a timeline.\n",
    "- We can also see the originating location of the source IP Address (in purple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='other_hosts_to_ips'></a>[Contents](#toc)\n",
    "## Have any other hosts been communicating with the same external IP address?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "hide_code()\n",
    "\n",
    "# Flatten our alert_ip_entities into a list of IPs for the query\n",
    "alert_ips = {'\\'{}\\''.format(i.Address) for i in alert_ip_entities}\n",
    "alert_host_ip_list = ','.join(alert_ips)\n",
    "\n",
    "az_ip_where = f'| where AllExtIPs in ({alert_host_ip_list})'\n",
    "\n",
    "az_net_query_by_pub_ip = az_net_analytics_query.format(where_clause=az_ip_where,\n",
    "                                                       start = security_alert.StartTimeUtc - timedelta(3),\n",
    "                                                       end = security_alert.StartTimeUtc + timedelta(1))\n",
    "print('getting data...')\n",
    "%kql -query az_net_query_by_pub_ip\n",
    "az_net_ext_comms_df = _kql_raw_result_.to_dataframe()\n",
    "\n",
    "# Get the IPs for our alert host\n",
    "inv_host_ips = [ent.Address for ent in inv_host_entity.private_ips]\n",
    "inv_host_ips += [ent.Address for ent in inv_host_entity.public_ips]\n",
    "\n",
    "alert_ips = [ip.Address for ip in alert_ip_entities]\n",
    "\n",
    "known_ips = inv_host_ips + alert_ips\n",
    "\n",
    "# Ips can be in one of 4 columns!\n",
    "def find_new_ips(known_ips, row):\n",
    "    new_ips = set()\n",
    "    if row.VMIPAddress and row.VMIPAddress not in known_ips:\n",
    "        new_ips.add(row.VMIPAddress)\n",
    "    if row.SrcIP and row.SrcIP not in known_ips:\n",
    "        new_ips.add(row.SrcIP)\n",
    "    if row.DestIP and row.DestIP not in known_ips:\n",
    "        new_ips.add(row.DestIP)\n",
    "    if row.PublicIPs:\n",
    "        for pub_ip in row.PublicIPs:\n",
    "            if pub_ip not in known_ips:\n",
    "                new_ips.add(pub_ip)\n",
    "    if new_ips:            \n",
    "        return list(new_ips)\n",
    "\n",
    "# now we look for any IP addresses that communicated with our host\n",
    "# and pull out any that we hadn't previously see (these would be in\n",
    "# alert_ip_entities)\n",
    "new_ips_all = az_net_ext_comms_df.apply(lambda x: find_new_ips(known_ips, x), axis=1).dropna()\n",
    "new_ips = set()\n",
    "for ip in [ip for item in new_ips_all for ip in item]:\n",
    "    new_ips.add(ip)\n",
    "display(Markdown(f'#### {len(new_ips)} previously unseen internal IP Addresses found in this data: {list(new_ips)}'))\n",
    "\n",
    "\n",
    "# We assume in this case that we are only getting 1 new IP\n",
    "# In reality we'd want to do this for every IP in new_ips\n",
    "\n",
    "# Get host details from Heartbeat and Azure NSG\n",
    "vm_ip = new_ips.pop()\n",
    "aznet_query = '''\n",
    "AzureNetworkAnalytics_CL \n",
    "| where PrivateIPAddresses_s has \\'{vm_ip}\\'\n",
    "| where ResourceType == 'NetworkInterface'\n",
    "| top 1 by TimeGenerated desc\n",
    "| project PrivateIPAddresses = PrivateIPAddresses_s, \n",
    "    PublicIPAddresses = PublicIPAddresses_s,\n",
    "    VirtualMachine = VirtualMachine_s\n",
    "| extend Host = split(VirtualMachine, '/')[-1]\n",
    "'''.format(vm_ip=vm_ip)\n",
    "%kql -query aznet_query\n",
    "az_net_df = _kql_raw_result_.to_dataframe()\n",
    "if len(az_net_df) > 0:\n",
    "    host_name = az_net_df['Host'].at[0]\n",
    "\n",
    "oms_heartbeat_query = '''\n",
    "Heartbeat\n",
    "| where ComputerIP == \\'{vm_ip}\\'\n",
    "| top 1 by TimeGenerated desc nulls last\n",
    "| project Computer, ComputerIP, OSType, OSMajorVersion, OSMinorVersion, ResourceId, RemoteIPCountry, \n",
    "RemoteIPLatitude, RemoteIPLongitude, SourceComputerId\n",
    "'''.format(vm_ip=vm_ip)\n",
    "%kql -query oms_heartbeat_query\n",
    "oms_heartbeat_df = _kql_raw_result_.to_dataframe()\n",
    "if len(oms_heartbeat_df) > 0:\n",
    "    host_name = oms_heartbeat_df['Computer'].at[0]\n",
    "    \n",
    "\n",
    "# Get the host entity and add this IP and system info to the \n",
    "try:\n",
    "    if not victim_host_entity:\n",
    "        victim_host_entity = mas.Host()\n",
    "        victim_host_entity.HostName = host_name\n",
    "except NameError:\n",
    "    victim_host_entity = mas.Host()\n",
    "    victim_host_entity.HostName = host_name\n",
    "\n",
    "def convert_to_ip_entities(ip_str):\n",
    "    ip_entities = []\n",
    "    if ip_str:\n",
    "        if ',' in ip_str:\n",
    "            addrs = ip_str.split(',')\n",
    "        elif ' ' in ip_str:\n",
    "            addrs = ip_str.split(' ')\n",
    "        else:\n",
    "            addrs = [ip_str]\n",
    "        for addr in addrs:\n",
    "            ip_entity = mas.IpAddress()\n",
    "            ip_entity.Address = addr.strip()\n",
    "            iplocation.lookup_ip(ip_entity=ip_entity)\n",
    "            ip_entities.append(ip_entity)\n",
    "    return ip_entities\n",
    "\n",
    "# Add this information to our inv_host_entity\n",
    "retrieved_pub_addresses = []\n",
    "if len(az_net_df) == 1:\n",
    "    priv_addr_str = az_net_df['PrivateIPAddresses'].loc[0]\n",
    "    victim_host_entity.properties['private_ips'] = convert_to_ip_entities(priv_addr_str)\n",
    "\n",
    "    pub_addr_str = az_net_df['PublicIPAddresses'].loc[0]\n",
    "    victim_host_entity.properties['public_ips'] = convert_to_ip_entities(pub_addr_str)\n",
    "    retrieved_pub_addresses = [ip.Address for ip in victim_host_entity.properties['public_ips']]\n",
    "    \n",
    "if len(oms_heartbeat_df) == 1:\n",
    "    if oms_heartbeat_df['ComputerIP'].loc[0]:\n",
    "        oms_address = oms_heartbeat_df['ComputerIP'].loc[0]\n",
    "        if oms_address not in retrieved_address:\n",
    "            ip_entity = mas.IpAddress()\n",
    "            ip_entity.Address = oms_address\n",
    "            iplocation.lookup_ip(ip_entity=ip_entity)\n",
    "            inv_host_entity.properties['public_ips'].append(ip_entity)\n",
    "        \n",
    "    victim_host_entity.OSFamily = oms_heartbeat_df['OSType'].loc[0]\n",
    "    victim_host_entity.AdditionalData['OSMajorVersion'] = oms_heartbeat_df['OSMajorVersion'].loc[0]\n",
    "    victim_host_entity.AdditionalData['OSMinorVersion'] = oms_heartbeat_df['OSMinorVersion'].loc[0]\n",
    "    victim_host_entity.AdditionalData['SourceComputerId'] = oms_heartbeat_df['SourceComputerId'].loc[0]\n",
    "\n",
    "print(f'Found New Host Entity {victim_host_entity.HostName}\\n')\n",
    "print(victim_host_entity)\n",
    "\n",
    "add_observation(Observation(caption=f'Second victim host identified {victim_host_entity.HostName}',\n",
    "                            description='Description of host entity shown in attachment.',\n",
    "                            item=victim_host_entity,\n",
    "                            link='other_hosts_to_ips'))\n",
    "\n",
    "\n",
    "# Plot our current set of IPs on a graph\n",
    "sns.set()\n",
    "from matplotlib import MatplotlibDeprecationWarning\n",
    "warnings.simplefilter(\"ignore\", category=MatplotlibDeprecationWarning)\n",
    "\n",
    "ip_graph = nx.DiGraph(id='IPGraph')\n",
    "\n",
    "def add_vm_node(graph, host_entity):\n",
    "    vm_name = host_entity.HostName\n",
    "    vm_ip = host_entity.private_ips[0].Address\n",
    "    vm_desc = f'{host_entity.HostName}'\n",
    "    ip_graph.add_node(vm_ip, name=vm_name, description=vm_desc,\n",
    "                      node_type='host')\n",
    "\n",
    "for ip_entity in alert_ip_entities:\n",
    "    if 'Location' in ip_entity:\n",
    "        ip_desc = f'{ip_entity.Address}\\n{ip_entity.Location.City}, {ip_entity.Location.CountryName}'\n",
    "    else:\n",
    "        ip_desc = 'unknown location'\n",
    "    ip_graph.add_node(ip_entity.Address, name=ip_entity.Address, description=ip_desc, node_type='ip')\n",
    "\n",
    "add_vm_node(ip_graph, inv_host_entity)\n",
    "add_vm_node(ip_graph, victim_host_entity)\n",
    "\n",
    "\n",
    "def add_edges(graph, row): \n",
    "    dest_ip = row.DestIP if row.DestIP else row.VMIPAddress\n",
    "    if row.SrcIP:\n",
    "        src_ip = row.SrcIP\n",
    "        ip_graph.add_edge(src_ip, dest_ip)\n",
    "    else:\n",
    "        for ip in row.PublicIPs:\n",
    "            src_ip = ip\n",
    "            ip_graph.add_edge(src_ip, dest_ip)\n",
    "\n",
    "    \n",
    "# Add edges from network data\n",
    "az_net_ext_comms_df.apply(lambda x: add_edges(ip_graph, x),axis=1)\n",
    "\n",
    "src_node = [n for (n, node_type) in\n",
    "            nx.get_node_attributes(ip_graph, 'node_type').items()\n",
    "            if node_type == 'ip']\n",
    "vm_nodes = [n for (n, node_type) in\n",
    "            nx.get_node_attributes(ip_graph, 'node_type').items()\n",
    "            if node_type == 'host']\n",
    "\n",
    "# now draw them in subsets  using the `nodelist` arg\n",
    "def show_ip_graph(ip_graph):\n",
    "    plt.rcParams['figure.figsize'] = (10, 10)\n",
    "    plt.margins(x=0.3, y=0.3)\n",
    "    plt.title('Comms between hosts and suspect IPs')\n",
    "    pos = nx.circular_layout(ip_graph)\n",
    "    nx.draw_networkx_nodes(ip_graph, pos, nodelist=src_node,\n",
    "                           node_color='red', alpha=0.5, node_shape='o')\n",
    "    nx.draw_networkx_nodes(ip_graph, pos, nodelist=vm_nodes,\n",
    "                           node_color='green', alpha=0.5, node_shape='s',\n",
    "                           s=400)\n",
    "    nlabels = nx.get_node_attributes(ip_graph, 'description')\n",
    "    nx.relabel_nodes(ip_graph, nlabels)\n",
    "    nx.draw_networkx_labels(ip_graph, pos, nlabels, font_size=15)\n",
    "    nx.draw_networkx_edges(ip_graph, pos, alpha=0.5, arrows=True, arrowsize=20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_code()\n",
    "show_ip_graph(ip_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- We have found a new IP address that our attacker has also been communicating with. We should investigate that machine.\n",
    "- From the name looks like a Windows host\n",
    "- There is also another public IP involved that has been taking to both hosts. We should look at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='other_host_investigate'></a>[Contents](#toc)\n",
    "# Other Hosts Communicating with IP\n",
    "Investigating the second host in the attack graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='host_logons'></a>[Contents](#toc)\n",
    "## Check Host Logons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hide_code()\n",
    "\n",
    "# Build query params for our new host\n",
    "from msticpy.nbtools.query_defns import DataFamily, DataEnvironment\n",
    "params_dict = {}\n",
    "params_dict.update(security_alert.query_params)\n",
    "params_dict['host_filter_eq'] = f'Computer has \\'{victim_host_entity.HostName}\\''\n",
    "params_dict['host_filter_neq'] = f'Computer !has \\'{victim_host_entity.HostName}\\''\n",
    "params_dict['host_name'] = victim_host_entity.HostName\n",
    "params_dict['start'] = security_alert.origin_time - timedelta(days=5)\n",
    "params_dict['end'] = security_alert.origin_time + timedelta(days=1)\n",
    "if victim_host_entity.OSFamily == 'Linux':\n",
    "    params_dict['data_family'] = DataFamily.LinuxSecurity\n",
    "    params_dict['path_separator'] = '/'\n",
    "else:\n",
    "    params_dict['data_family'] = DataFamily.WindowsSecurity\n",
    "    params_dict['path_separator'] = '\\\\'\n",
    "\n",
    "# We might want to set the query times manually instead of hard-coding them (to -5days, +1 day)\n",
    "# # set the origin time to the time of our alert\n",
    "# logon_query_times = mas.QueryTime(units='day', origin_time=security_alert.origin_time,\n",
    "#                                   before=5, after=1, max_before=20, max_after=20)\n",
    "# logon_query_times.display()\n",
    "\n",
    "\n",
    "# We know we'll get back a million SYSTEM logins so use clustering again to \n",
    "# squash down the repetitive events\n",
    "from msticpy.sectools.eventcluster import dbcluster_events, add_process_features, _string_score\n",
    "print('getting data...', end=' ')\n",
    "host_logons = qry.list_host_logons(**params_dict)\n",
    "\n",
    "# Use Account name, LogonId and the hour of logon as a cluster features\n",
    "if len(host_logons) > 0:\n",
    "    logon_features = host_logons.copy()\n",
    "    logon_features['AccountNum'] = host_logons.apply(lambda x: _string_score(x.Account), axis=1)\n",
    "    logon_features['LogonIdNum'] = host_logons.apply(lambda x: _string_score(x.TargetLogonId), axis=1)\n",
    "    logon_features['LogonHour'] = host_logons.apply(lambda x: x.TimeGenerated.hour, axis=1)\n",
    "\n",
    "    # you might need to play around with the max_cluster_distance parameter.\n",
    "    # decreasing this gives more clusters.\n",
    "    (clus_logons, _, _) = dbcluster_events(data=logon_features, time_column='TimeGenerated',\n",
    "                                           cluster_columns=['AccountNum',\n",
    "                                                            'LogonType'],\n",
    "                                           max_cluster_distance=0.0001)\n",
    "\n",
    "logon_by_type = (host_logons[['Account', 'LogonType', 'EventID']]\n",
    "                .groupby(['Account','LogonType']).count().unstack()\n",
    "                .fillna(0)\n",
    "                .style\n",
    "                .background_gradient(cmap='viridis', low=.5, high=0)\n",
    "                .format(\"{0:0>3.0f}\"))\n",
    "\n",
    "# Add an observation to our status results\n",
    "add_observation(Observation(caption='RDP Logons seen for victim #2',\n",
    "                            description='Logons by logon type.',\n",
    "                            item=logon_by_type,\n",
    "                            link='victim2_logon_types'))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_code()\n",
    "display(Markdown('### Classification of Logon Types by Account'))\n",
    "display(Markdown('Color map - yellow shows highest for each logon type.'))\n",
    "logon_by_type = (host_logons[['Account', 'LogonType', 'EventID']]\n",
    "                .groupby(['Account','LogonType']).count().unstack()\n",
    "                .fillna(0)\n",
    "                .style\n",
    "                .background_gradient(cmap='viridis', low=.5, high=0)\n",
    "                .format(\"{0:0>3.0f}\"))\n",
    "display(logon_by_type)\n",
    "key = '**LogonType key** = {}'.format('; '.join([f'{k}: {v}' for k,v in mas.nbdisplay._WIN_LOGON_TYPE_MAP.items()]))\n",
    "display(Markdown(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_code()\n",
    "display(Markdown('### Logon Timeline'))\n",
    "nbdisp.display_timeline(data=host_logons,\n",
    "                         overlay_data=host_logons.query('LogonType == 10'),\n",
    "                         alert=security_alert, \n",
    "                         source_columns=['Account', 'LogonType', 'TimeGenerated'],\n",
    "                         title='All Host Logons (RDP Logons in green)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_code()\n",
    "display(Markdown('### Failed Logons'))\n",
    "\n",
    "failedLogons = qry.list_host_logon_failures(**params_dict)\n",
    "if failedLogons.shape[0] == 0:\n",
    "    display(print('No logon failures recorded for this host between {security_alert.start} and {security_alert.start}'))\n",
    "else:\n",
    "    display(failedLogons)\n",
    "    add_observation(Observation(caption='Logon failures seen for victim #2',\n",
    "                            description=f'{len(failedLogons)} Logons seen.',\n",
    "                            item=failedLogons,\n",
    "                            link='failed_logons'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- We see some Network and RemoteInteractive logons - some occuring around the time leading up to the original alert.\n",
    "- From the failed logons it looks like a few attempts were made to logon to the account name \"ian\".\n",
    "- Once the logon succeeded our attacker logged in using remote desktop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='examine_win_logon_sess'></a>[Contents](#toc)\n",
    "## Examine a Logon Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a Logon ID To Examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hide_code()\n",
    "\n",
    "import re\n",
    "dist_logons = clus_logons.sort_values('TimeGenerated')[['TargetUserName', 'TimeGenerated', \n",
    "                                                        'LastEventTime', 'LogonType', \n",
    "                                                        'ClusterSize']]\n",
    "# Get the clustered logons as input to our selection list\n",
    "items = dist_logons.apply(lambda x: (f'{x.TargetUserName}:    '\n",
    "                                     f'(logontype={x.LogonType})   '\n",
    "                                     f'timerange={x.TimeGenerated} - {x.LastEventTime}    '\n",
    "                                     f'count={x.ClusterSize}'),\n",
    "                          axis=1).values.tolist()\n",
    "\n",
    "# Given the string value chosen from the select list\n",
    "# get the logon cluster that this corresponds to\n",
    "def get_selected_logon_cluster(selected_item):\n",
    "    acct_match = re.search(r'(?P<acct>[^:]+):\\s+\\(logontype=(?P<l_type>[^)]+)', selected_item)\n",
    "    if acct_match:\n",
    "        acct = acct_match['acct']\n",
    "        l_type = int(acct_match['l_type'])\n",
    "        return host_logons.query('TargetUserName == @acct and LogonType == @l_type')\n",
    "\n",
    "# Given the string value chosen from the LogonInstances select list\n",
    "# get the logon instance that this corresponds to\n",
    "def get_selected_logon(selected_item):\n",
    "    logon_list_regex = r'''\n",
    "(?P<acct>[^:]+):\\s+\n",
    "\\(logontype=(?P<logon_type>[^)]+)\\)\\s+\n",
    "\\(timestamp=(?P<time>[^)]+)\\)\\s+\n",
    "logonid=(?P<logonid>[0-9a-fx)]+)\n",
    "'''\n",
    "    acct_match = re.search(logon_list_regex, selected_item, re.VERBOSE)\n",
    "    if acct_match:\n",
    "        acct = acct_match['acct']\n",
    "        logon_type = int(acct_match['logon_type'])\n",
    "        time_stamp = pd.to_datetime(acct_match['time'])\n",
    "        logon_id = acct_match['logonid']\n",
    "        return host_logons.query('TargetUserName == @acct and LogonType == @logon_type'\n",
    "                                 ' and TargetLogonId == @logon_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_code()    \n",
    "logon_wgt = mas.SelectString(description='Select logon cluster to examine', \n",
    "                             item_list=items, height='200px', width='100%', auto_display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- Select the logon account and type that we're interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='process_clustering'></a>[Contents](#toc)\n",
    "## Unusual Processes on Host - Clustering\n",
    "Sometimes you don't have a source process to work with. Other times it's just useful to see what else is going on on the host. This section retrieves all processes on the host within the time bounds\n",
    "set in the query times widget.\n",
    "\n",
    "Usually though, the results return a lot of very repetitive and uninteresting system processes so we attempt to cluster these to make the view easier to negotiate. \n",
    "To do this we process the raw event list output to extract a few features that render strings (such as commandline)into numerical values. The default below uses the following features:\n",
    "- commandLineTokensFull - this is a count of common delimiters in the commandline \n",
    "  (given by this regex r'[\\s\\-\\\\/\\.,\"\\'|&:;%$()]'). The aim of this is to capture the commandline structure while ignoring variations on what is essentially the same pattern (e.g. temporary path GUIDs, target IP or host names, etc.)\n",
    "- pathScore - this sums the ordinal (character) value of each character in the path (so /bin/bash and /bin/bosh would have similar scores).\n",
    "- isSystemSession - 1 if this is a root/system session, 0 if anything else.\n",
    "\n",
    "Then we run a clustering algorithm (DBScan in this case) on the process list. The result groups similar (noisy) processes together and leaves unique process patterns as single-member clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_code()\n",
    "\n",
    "# Get the logon cluster corresponding to the value selected in the\n",
    "# previous list\n",
    "selected_logon_cluster = get_selected_logon_cluster(logon_wgt.value)\n",
    "\n",
    "# Calculate time range based on the logons from previous section\n",
    "logon_time = selected_logon_cluster['TimeGenerated'].min()\n",
    "last_logon_time = selected_logon_cluster['TimeGenerated'].max()\n",
    "time_diff_delta = (last_logon_time - logon_time)\n",
    "\n",
    "# set the origin time to the time of our alert\n",
    "params_dict['start'] = logon_time - timedelta(seconds=(1 * 60 * 60))\n",
    "params_dict['end'] = logon_time + time_diff_delta + timedelta(seconds=(2 * 60 * 60))\n",
    "\n",
    "# Meanwhile we would probably want to cluster the processed in the same way\n",
    "# We'd certainly need to do this if we end up looking at a system logon session\n",
    "# but for discrete user logons we don't usually needed it.\n",
    "from msticpy.sectools.eventcluster import dbcluster_events, add_process_features\n",
    "print('Getting process events...', end='')\n",
    "processes_on_host = qry.list_processes(**params_dict)\n",
    "print('done')\n",
    "print('Clustering...', end='')\n",
    "feature_procs = add_process_features(input_frame=processes_on_host,\n",
    "                                     path_separator=params_dict['path_separator'])\n",
    "\n",
    "feature_procs['accountNum'] = feature_procs.apply(lambda x: _string_score(x.Account), axis=1)\n",
    "# you might need to play around with the max_cluster_distance parameter.\n",
    "# decreasing this gives more clusters.\n",
    "(clus_events, dbcluster, x_data) = dbcluster_events(data=feature_procs,\n",
    "                                                    cluster_columns=['commandlineTokensFull', \n",
    "                                                                     'pathScore',\n",
    "                                                                     'accountNum',\n",
    "                                                                     'isSystemSession'],\n",
    "                                                    max_cluster_distance=0.0001)\n",
    "print('done')\n",
    "print('Number of input events:', len(feature_procs))\n",
    "print('Number of clustered events:', len(clus_events))\n",
    "(clus_events.sort_values('TimeGenerated')[['TimeGenerated', 'LastEventTime',\n",
    "                                          'NewProcessName', 'CommandLine', \n",
    "                                          'ClusterSize', 'commandlineTokensFull',\n",
    "                                          'pathScore', 'isSystemSession']]\n",
    "    .sort_values('ClusterSize', ascending=False))\n",
    "print('done')\n",
    "\n",
    "\n",
    "display(Markdown('### View processes used in login session'))\n",
    "\n",
    "# Callback for Select widget to display processes belonging to this\n",
    "# logon session\n",
    "def view_logon_sess(x=''):\n",
    "    global selected_logon\n",
    "    selected_logon = get_selected_logon(x)\n",
    "    display(selected_logon)\n",
    "    logonId = selected_logon['TargetLogonId'].iloc[0]\n",
    "    sess_procs = (processes_on_host.query('TargetLogonId == @logonId | SubjectLogonId == @logonId')\n",
    "                                          [['NewProcessName', 'CommandLine', 'TargetLogonId']]\n",
    "                  .drop_duplicates())\n",
    "    display(sess_procs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_code()\n",
    "items = selected_logon_cluster.sort_values('TimeGenerated').apply(lambda x: (f'{x.TargetUserName}:    '\n",
    "                                        f'(logontype={x.LogonType})   '\n",
    "                                        f'(timestamp={x.TimeGenerated})    '\n",
    "                                        f'logonid={x.TargetLogonId}'),\n",
    "                             axis=1).values.tolist()\n",
    "sess_w = widgets.Select(options=items, description='Select logon instance to examine', **WIDGET_DEFAULTS)\n",
    "    \n",
    "interactive(view_logon_sess, x=sess_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- First session listed doesn't look interesting \n",
    "- but the second shows:\n",
    "  - Accounts being added\n",
    "  - Accounts added to local Administrators group\n",
    "  - Permissions removed on RPD/Terminal services logon\n",
    "  - Scheduled task being installed as a means of persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for unusual Event Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_code()\n",
    "display(Markdown('### EventTypes for logon sessions in selected time interval'))\n",
    "\n",
    "# We might want to restrict this to just the account that we suspect but for \n",
    "# servers with limited numbers of user logons we can show them all.\n",
    "all_events_base_qry = '''\n",
    "SecurityEvent\n",
    "| where Computer =~ '{host}'\n",
    "| where TimeGenerated >= datetime({start})\n",
    "| where TimeGenerated <= datetime({end})\n",
    "| where {where_filter}\n",
    "'''\n",
    "all_events_qry = all_events_base_qry.format(host=params_dict['host_name'],\n",
    "                                            start=params_dict['start'],\n",
    "                                            end=params_dict['end'],\n",
    "                                            where_filter='EventID != 4688 and EventID != 4624')\n",
    "\n",
    "%kql -query all_events_qry\n",
    "all_events_df = _kql_raw_result_.to_dataframe()\n",
    "display(all_events_df[['Account', 'Activity', 'TimeGenerated']]\n",
    "        .groupby(['Account', 'Activity']).count())\n",
    "\n",
    "add_observation(Observation(caption='System account modifications during attack.',\n",
    "                            description='Count of event types seen on system',\n",
    "                            item=all_events_df[['Account', 'Activity', 'TimeGenerated']].groupby(['Account', 'Activity']).count(),\n",
    "                            link='other_win_events'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Some unusual events associated with our compromised account session:\n",
    "- Logon failure\n",
    "- Account creation\n",
    "- Password reset\n",
    "- Security group changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='o365'></a>[Contents](#toc)\n",
    "# Office 365 Activity\n",
    "### Check our Office Subscription for anything suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the origin time to the time of our alert\n",
    "\n",
    "o365_query_times = mas.QueryTime(units='day', origin_time=security_alert.origin_time,\n",
    "                                 before=3, after=3, max_before=20, max_after=20)\n",
    "o365_query_times.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hide_code()\n",
    "# default to 1 hour before and 10 hours after - should set this\n",
    "# in widget\n",
    "# o365_start = security_alert.origin_time - timedelta(seconds=(60 * 60))\n",
    "# o365_end = security_alert.origin_time + timedelta(seconds=(10 * 60 * 60))\n",
    "o365_start = o365_query_times.start\n",
    "o365_end = o365_query_times.end\n",
    "\n",
    "print('Running queries...', end=' ')\n",
    "# Query Definitions\n",
    "\n",
    "# Look for AAD events that are significant changes like\n",
    "# adding users, changing roles, etc.\n",
    "# Note: we need to switch this to use the AAD log.\n",
    "ad_changes_query = '''\n",
    "OfficeActivity\n",
    "| where TimeGenerated >= datetime({start})\n",
    "| where TimeGenerated <= datetime({end})\n",
    "| where RecordType == 'AzureActiveDirectory'\n",
    "| where Operation in ('Add service principal.',\n",
    "                      'Change user password.', \n",
    "                      'Add user.', \n",
    "                      'Add member to role.')\n",
    "| where UserType == 'Regular' \n",
    "| project OfficeId, TimeGenerated, Operation, OrganizationId, \n",
    "          OfficeWorkload, ResultStatus, OfficeObjectId, \n",
    "          UserId = tolower(UserId), ClientIP, ExtendedProperties\n",
    "'''.format(start = o365_start, end=o365_end)\n",
    "%kql -query ad_changes_query\n",
    "ad_changes_df = _kql_raw_result_.to_dataframe()\n",
    "\n",
    "# Get all office activity in the specified time range\n",
    "# NOTE - expensive for large orgs\n",
    "office_ops_query = '''\n",
    "OfficeActivity\n",
    "| where TimeGenerated >= datetime({start})\n",
    "| where TimeGenerated <= datetime({end})\n",
    "| where RecordType in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n",
    "| extend UserAgent = extractjson(\"$[0].Value\", ExtendedProperties, typeof(string))\n",
    "| union (\n",
    "    OfficeActivity \n",
    "    | where TimeGenerated >= datetime({start})\n",
    "    | where TimeGenerated <= datetime({end})\n",
    "    | where RecordType !in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n",
    ")\n",
    "| where UserType == 'Regular'\n",
    "'''.format(start = o365_start, end=o365_end)\n",
    "%kql -query office_ops_query\n",
    "office_ops_df = _kql_raw_result_.to_dataframe()\n",
    "\n",
    "# Similar to previous query but we summarize counts in 1 hour bins\n",
    "office_ops_summary_query = '''\n",
    "let timeRange=ago(30d);\n",
    "let officeAuthentications = OfficeActivity\n",
    "| where TimeGenerated >= timeRange\n",
    "| where RecordType in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n",
    "| extend UserAgent = extractjson(\"$[0].Value\", ExtendedProperties, typeof(string))\n",
    "| where Operation == \"UserLoggedIn\";\n",
    "officeAuthentications\n",
    "| union (\n",
    "    OfficeActivity \n",
    "    | where TimeGenerated >= timeRange\n",
    "    | where RecordType !in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n",
    ")\n",
    "| where UserType == 'Regular'\n",
    "| extend RecordOp = strcat(RecordType, '-', Operation)\n",
    "| summarize OpCount=count() by RecordType, Operation, UserId, UserAgent, ClientIP, bin(TimeGenerated, 1h)\n",
    "// render timeline\n",
    "'''.format(start = o365_start, end=o365_end)\n",
    "%kql -query office_ops_summary_query\n",
    "office_ops_summary_df = _kql_raw_result_.to_dataframe()\n",
    "\n",
    "\n",
    "# office_logons_query = '''\n",
    "# let timeRange=ago(30d);\n",
    "# let officeAuthentications = OfficeActivity\n",
    "# | where TimeGenerated >= timeRange\n",
    "# | where RecordType in (\"AzureActiveDirectoryAccountLogon\", \"AzureActiveDirectoryStsLogon\")\n",
    "# | extend UserAgent = extractjson(\"$[0].Value\", ExtendedProperties, typeof(string))\n",
    "# | where Operation == \"UserLoggedIn\";\n",
    "# let lookupWindow = 1d;\n",
    "# let lookupBin = lookupWindow / 2.0; \n",
    "# officeAuthentications | project-rename Start=TimeGenerated\n",
    "# | extend TimeKey = bin(Start, lookupBin)\n",
    "# | join kind = inner (\n",
    "#     officeAuthentications\n",
    "#     | project-rename End=TimeGenerated\n",
    "#     | extend TimeKey = range(bin(End - lookupWindow, lookupBin), bin(End, lookupBin), lookupBin)\n",
    "#     | mvexpand TimeKey to typeof(datetime)\n",
    "# ) on UserAgent, TimeKey\n",
    "# | project timeSpan = End - Start, UserId, ClientIP , UserAgent , Start, End\n",
    "# | summarize dcount(ClientIP) by  UserAgent\n",
    "# | where dcount_ClientIP > 1\n",
    "# | join kind=inner (  \n",
    "# officeAuthentications\n",
    "# | summarize minTime=min(TimeGenerated), maxTime=max(TimeGenerated) by UserId, UserAgent, ClientIP\n",
    "# ) on UserAgent\n",
    "# '''\n",
    "# %kql -query office_logons_query\n",
    "# office_logons_df = _kql_raw_result_.to_dataframe()\n",
    "\n",
    "print('done.')\n",
    "\n",
    "# Extract and display activities for a given IP \n",
    "def show_o365_activity_for_ip(office_ops_df, ip):\n",
    "    susp_o365_activities = office_ops_df[office_ops_df['ClientIP'] == ip.Address]\n",
    "    susp_o365_summ = (office_ops_df[office_ops_df['ClientIP'] == ip.Address]\n",
    "                         [['OfficeId', 'UserId', 'RecordType', 'Operation']]\n",
    "                         .groupby(['UserId', 'RecordType', 'Operation']).count()\n",
    "                         .rename(columns={'OfficeId': 'OperationCount'}))\n",
    "    \n",
    "    display(Markdown(f'#### Activity for {ip.Address}'))\n",
    "    if len(susp_o365_summ) > 0:\n",
    "        display(susp_o365_summ)\n",
    "        add_observation(Observation(caption=f'O365 activity from suspected attacker IP {ip.Address}',\n",
    "                                    description=f'Summarized operation count for each user/service/operation type',\n",
    "                                    item=susp_o365_summ,\n",
    "                                    link='o365_match_ip'))\n",
    "    else:\n",
    "        display(Markdown('No activity detected'))\n",
    "\n",
    "# Extract and display a timeline of activities for a given IP \n",
    "def show_o365_timeline_for_ip(office_ops_df, ip):\n",
    "    suspect_ip_ops = office_ops_df[office_ops_df['ClientIP'] == ip.Address]\n",
    "    \n",
    "    display(Markdown(f'#### Timeline of operations originating from suspect IP Address: {ip.Address}'))\n",
    "    if len(suspect_ip_ops) == 0:\n",
    "        display(Markdown('No activity detected'))\n",
    "        return\n",
    "    sel_op_type='FileDownloaded'\n",
    "    nbdisp.display_timeline(data=suspect_ip_ops, title=f'Operations from {ip.Address} (all=blue, {sel_op_type}=green)',\n",
    "                            overlay_data=suspect_ip_ops.query('Operation == @sel_op_type'),\n",
    "                            height=200,\n",
    "                            source_columns=['UserId', 'RecordType', 'Operation'])\n",
    "    \n",
    "    # Uncomment line below to see all activity\n",
    "    # display(suspect_ip_ops.sort_values('TimeGenerated', ascending=True).head())\n",
    "\n",
    "# We're going to summarize activity counts in 10 second windows\n",
    "# To try to identify high freq/automated events.\n",
    "# It's more efficient to do this in Kusto/Log Analytics but since\n",
    "# we already have the data we can do it here. for large orgs you'd\n",
    "# almost certainly want logAnalytics to do this.\n",
    "timed_slice_ops = office_ops_df[['RecordType', 'TimeGenerated', 'Operation',\n",
    "       'OrganizationId', 'UserType', 'OfficeWorkload',\n",
    "       'ResultStatus', 'OfficeObjectId', 'UserId', 'ClientIP', 'Start_Time']]\n",
    "timed_slice_ops2 = timed_slice_ops.set_index('TimeGenerated')\n",
    "\n",
    "hi_freq_ops = (timed_slice_ops2[['UserId', 'ClientIP', 'Operation', 'RecordType']]\n",
    "                .groupby(['UserId', 'ClientIP', 'RecordType', 'Operation']).resample('10S').count()\n",
    "                .query('RecordType > 10')\n",
    "                .drop(['ClientIP', 'UserId', 'RecordType'], axis=1)\n",
    "                .assign(OpsPerSec = lambda x: x.Operation / 10)\n",
    "                .rename(columns={'Operation': 'Operation Count'}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any IP Addresses in our alert IPs that match Office Activity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_code()\n",
    "unique_ips = set()\n",
    "for susp_ip in alert_ip_entities:\n",
    "    if susp_ip.Address in unique_ips:\n",
    "        continue\n",
    "    unique_ips.add(susp_ip.Address)\n",
    "    display(Markdown(f'### IP: {susp_ip.Address}'))\n",
    "    show_o365_activity_for_ip(office_ops_df, susp_ip)\n",
    "    show_o365_timeline_for_ip(office_ops_df, susp_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence of automated or bulk uploads/downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(hi_freq_ops) > 0:\n",
    "    display(hi_freq_ops)\n",
    "    add_observation(Observation(caption=f'O365 bulk/high freq operations seen',\n",
    "                                    description=f'Summarized operation count bulk actions',\n",
    "                                    item=hi_freq_ops,\n",
    "                                    link='o356_high_freq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- Logon failures and success originating from our attacker IP Address\n",
    "- File download activity\n",
    "- We also confirm what looks like data exfiltration with another check that looks for high frequency operations and see the  same IP as the originator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for observation in observation_list.values():\n",
    "    display_observation(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id='appendices'></a>[Contents](#toc)\n",
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('List of current DataFrames in Notebook')\n",
    "print('-' * 50)\n",
    "current_vars = list(locals().keys())\n",
    "for var_name in current_vars:\n",
    "    if isinstance(locals()[var_name], pd.DataFrame) and not var_name.startswith('_'):\n",
    "        print(var_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "tags": [
     "todo"
    ]
   },
   "source": [
    "## Saving Data to Excel\n",
    "To save the contents of a pandas DataFrame to an Excel spreadsheet\n",
    "use the following syntax\n",
    "```\n",
    "writer = pd.ExcelWriter('myWorksheet.xlsx')\n",
    "my_data_frame.to_excel(writer,'Sheet1')\n",
    "writer.save()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
    "name": "python36",
    "display_name": "Python 3.6",
    "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "318.996px",
    "width": "320.994px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "270px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "406.193px",
    "left": "1468.4px",
    "right": "20px",
    "top": "120px",
    "width": "456.572px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
